{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12-07.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPaD1qcZP9C8yMxWdhF1uOH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julija-dmrk/data-mining/blob/main/12_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UYMfIiQWsOz"
      },
      "source": [
        "**Logistic Regression**\n",
        "\n",
        "Logistic regression and its extensions, like multinomial logistic\n",
        "regression, allow us to predict the probability that an observation is of a certain class\n",
        "using a straightforward and well-understood approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXFIrmVVXLkP"
      },
      "source": [
        "1. Training a Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LongqFEVUevd",
        "outputId": "1d180d8e-355c-490e-d757-e9c8974f52ac"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load data with only two classes\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data[:100,:]\n",
        "target = iris.target[:100]\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "# Create logistic regression object\n",
        "logistic_regression = LogisticRegression(random_state=0)\n",
        "# Train model\n",
        "model = logistic_regression.fit(features_standardized, target)\n",
        "\n",
        "#a logistic regression is actually a widely used binary classifier (i.e., the target vector can only take two values)\n",
        "#linear model is included in logistic(sigmoid) function\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK3-aPe3Y6LT",
        "outputId": "171dd95f-0e67-46d8-abb2-5cc50e2417b4"
      },
      "source": [
        "# Create new observation\n",
        "new_observation = [[.5, .5, .5, .5]]\n",
        "# Predict class\n",
        "model.predict(new_observation)\n",
        "\n",
        "# View predicted probabilities\n",
        "model.predict_proba(new_observation)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.17738424, 0.82261576]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03OMN1RlZmxb"
      },
      "source": [
        "2.Training a multiclass classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWnKgAFdZiZd"
      },
      "source": [
        "#If there are >2 classes, we neeed to train a classifier model\n",
        "# Create one-vs-rest logistic regression object\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "target = iris.target\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "# Create one-vs-rest logistic regression object\n",
        "logistic_regression = LogisticRegression(random_state=0, multi_class=\"ovr\")\n",
        "# Train model\n",
        "model = logistic_regression.fit(features_standardized, target)\n",
        "\n",
        "#logistic regressions are only binary classifiers, meaning they cannot handle\n",
        "#target vectors with more than two classes. However, two clever extensions to logistic\n",
        "#regression do just that. First, in one-vs-rest logistic regression (OVR) a separate model is\n",
        "#trained for each class predicted whether an observation is that class or not (thus making it\n",
        "#a binary classification problem)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbopFvDQaPJY"
      },
      "source": [
        "3.Reducing Variance Through Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WmsRtrzaNuN",
        "outputId": "1ef61c2f-06d8-434b-c459-3b2af3d48dbc"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "target = iris.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "# Create decision tree regression object\n",
        "logistic_regression = LogisticRegressionCV(\n",
        "penalty='l2', Cs=10, random_state=0, n_jobs=-1)\n",
        "# Train model\n",
        "model = logistic_regression.fit(features_standardized, target)\n",
        "\n",
        "#Regularization is a method of penalizing complex models to reduce their variance.\n",
        "#Specifically, a penalty term is added to the loss function we are trying to minimize,\n",
        "#typically the L1 and L2 penalties\n",
        "\n",
        "#To reduce variance while using logistic\n",
        "#regression, we can treat C as a hyperparameter to be tuned to find the value of C that\n",
        "#creates the best model. In scikit-learn we can use the LogisticRegressionCV class to\n",
        "#efficiently tune C\n",
        "\n",
        "#Unfortunately, LogisticRegressionCV does not allow us to search over different\n",
        "#penalty terms"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balance', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT6tac4bC72"
      },
      "source": [
        "4. Training a Classifier on Very Large Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv7bIph3bGX2"
      },
      "source": [
        "#using the stochastic average gradient (SAG) solver (much faster model training),\n",
        "#but sensitive to feature scaling\n",
        "\n",
        "# Load libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "target = iris.target\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "# Create logistic regression object\n",
        "logistic_regression = LogisticRegression(random_state=0, solver=\"sag\")\n",
        "# Train model\n",
        "model = logistic_regression.fit(features_standardized, target)\n",
        "\n",
        "# Most of the time scikit-learn will select the best solver (techniques for training logistic regression)\n",
        "# automatically for us or warn us that we cannot do something with that solver\n",
        "\n",
        "# stochastic average gradient descent allows us to\n",
        "#train a model much faster than other solvers when our data is very large. However, it is\n",
        "#also very sensitive to feature scaling, so standardizing our features is particularly\n",
        "#important. We can set our learning algorithm to use this solver by setting\n",
        "#solver='sag'.\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBUEqgd7bRNI"
      },
      "source": [
        "5. Handling Imbalanced Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCT7ozuMbOqz",
        "outputId": "32518704-ea3a-4473-a864-9bbae1d96d3d"
      },
      "source": [
        "#If we have highly imbalanced classes and have not addressed it during preprocessing, \n",
        "#we have the option of using the class_weight parameter to weight the classes to make\n",
        "#certain we have a balanced mix of each class. \n",
        "#Specifically, the balanced argument will automatically weigh classes\n",
        "#inversely proportional to their frequency\n",
        "\n",
        "\n",
        "# Load libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "target = iris.target\n",
        "# Make class highly imbalanced by removing first 40 observations\n",
        "features = features[40:,:]\n",
        "target = target[40:]\n",
        "# Create target vector indicating if class 0, otherwise 1\n",
        "target = np.where((target == 0), 0, 1)\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "# Create decision tree regression object\n",
        "logistic_regression = LogisticRegression(random_state=0, class_weight=\"balance\")\n",
        "# Train model\n",
        "model = logistic_regression.fit(features_standardized, target)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balance', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}